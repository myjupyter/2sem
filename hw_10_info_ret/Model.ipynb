{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bc31901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import pymorphy2\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a989d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jupiter/workbench/python/2sem/hw_10_info_ret...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\n",
    "    'stopwords',\n",
    "    download_dir=os.getcwd(),\n",
    ")\n",
    "nltk.data.path.append(os.getcwd())\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "stop_words.add('свой')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98da2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999c37e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = '[^A-Za-z\\sаАбБвВгГдДеЕёЁжЖзЗиИйЙкКлЛмМнНоОпПрРсСтТуУфФхХцЦчЧшШщЩъЪыЫьЬэЭюЮяЯ]'\n",
    "regexRussian = '[^\\sаАбБвВгГдДеЕёЁжЖзЗиИйЙкКлЛмМнНоОпПрРсСтТуУфФхХцЦчЧшШщЩъЪыЫьЬэЭюЮяЯ]'\n",
    "rgRu = re.compile(regexRussian)\n",
    "rg = re.compile(regex)\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc00a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_convert_dict = {\n",
    "    'ADVB': 'ADV',\n",
    "    'ADJF': 'ADJ',\n",
    "    'NPRO': 'NOUN'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbdeb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_condition(token):\n",
    "    return all([\n",
    "        token.ent_type == 0,\n",
    "        rgRu.match(token.text) is None,\n",
    "        token.is_alpha,\n",
    "        not token.like_num,\n",
    "        not token.like_email,\n",
    "    ])\n",
    "\n",
    "def tokenize_spacy(text: str):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if tokenization_condition(token):\n",
    "            token = morph.parse(token.text)[0].normal_form\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_gensim(text: str):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if tokenization_condition(token):\n",
    "            tag = morph.parse(token.text)[0]\n",
    "            token = tag.normal_form\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_gensim_pretrained(text: str):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if tokenization_condition(token):\n",
    "            tag = morph.parse(token.text)[0]\n",
    "            token = tag.normal_form\n",
    "            pos = tag.tag.POS\n",
    "            if pos_convert_dict.get(pos) is not None:\n",
    "                pos = pos_convert_dict[pos]\n",
    "            if pos is not None:\n",
    "                token = '_'.join([token, pos])\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def process_fn(text: str):\n",
    "    doc = nlp(text)\n",
    "    text = re.sub(regex, \"\", text).lower()\n",
    "    new_text = []\n",
    "    for word in text.split(' '):\n",
    "        if word not in stop_words:\n",
    "            word = morph.parse(word)[0].normal_form\n",
    "            new_text.append(word)\n",
    "    text = ' '.join(new_text)\n",
    "    return text\n",
    "\n",
    "def process_fn_gensim(text: str):\n",
    "    text = ' '.join(re.sub(regexRussian, \"\", text).lower().split())\n",
    "    new_text = []\n",
    "    for word in text.split(' '):\n",
    "        if word not in stop_words:\n",
    "            tag = morph.parse(word)[0]\n",
    "            word = tag.normal_form\n",
    "            pos = tag.tag.POS\n",
    "            if pos is None:\n",
    "                continue\n",
    "            if pos == 'ADVB':\n",
    "                pos = 'ADV'\n",
    "            if pos == 'ADJF':\n",
    "                pos = 'ADJ'\n",
    "            word = '_'.join([word, pos])\n",
    "            new_text.append(word)\n",
    "    text = ' '.join(new_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1ba862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    def __init__(self, filepath, process_fn = process_fn):\n",
    "        with open(filepath) as file:\n",
    "            data = json.loads(file.read())\n",
    "        self.__labels = np.array([])\n",
    "        self.__raw_text = []\n",
    "        for d in data['data']:\n",
    "            self.__labels = np.append(self.__labels, d['evaluation'])\n",
    "            self.__raw_text.append(d['speech'])\n",
    "        \n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self.__labels\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        return self.__raw_text\n",
    "    \n",
    "    @property\n",
    "    def text_and_labels(self):\n",
    "        return list(zip(self.text, self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "131174f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbVectorizer:\n",
    "    def __init__(self, preprocessor = None, tokenizer = None, stop_words = None):\n",
    "        self.__preprocessor = preprocessor\n",
    "        if preprocessor is None:\n",
    "            self.__preprocessor = lambda x : x\n",
    "            \n",
    "        self.__tokenizer = tokenizer\n",
    "        if tokenizer is None:\n",
    "            self.__tokenizer = lambda x : x.split()\n",
    "            \n",
    "        self.__stop_words = stop_words\n",
    "        if stop_words is None:\n",
    "            self.__stop_words = {}\n",
    "            \n",
    "    def __process(self, raw_documents, y = None):\n",
    "        raw_documents = list(map(self.__preprocessor, raw_documents))\n",
    "        raw_documents = list(map(lambda x: ' '.join(list(filter(lambda x: x not in self.__stop_words, x.split()))), raw_documents))\n",
    "        tokens = list(map(self.__tokenizer, raw_documents))\n",
    "        return tokens\n",
    "        \n",
    "    def fit(self, raw_documents, y = None):\n",
    "        self.__fit(raw_documents)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, raw_documents):\n",
    "        return self.__transform(raw_documents)\n",
    "    \n",
    "    def __fit(self, raw_documents):\n",
    "        tokens = self.__process(raw_documents)\n",
    "        docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokens)]\n",
    "        self.__model = Doc2Vec(docs, vector_size=300, window=5, min_count=0, workers=4)\n",
    "        \n",
    "    def __transform(self, tokens: list):\n",
    "        vec = []\n",
    "        for token in self.__process(tokens):\n",
    "            vec.append(self.__model.infer_vector(token))\n",
    "        return np.array(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ef18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedVectorizer:\n",
    "    ruscorpora_model = gensim.downloader.load('word2vec-ruscorpora-300')\n",
    "    def __init__(self, preprocessor = None, tokenizer = None, stop_words = None):\n",
    "        self.__preprocessor = preprocessor\n",
    "        if preprocessor is None:\n",
    "            self.__preprocessor = lambda x : x\n",
    "            \n",
    "        self.__tokenizer = tokenizer\n",
    "        if tokenizer is None:\n",
    "            self.__tokenizer = lambda x : x.split()\n",
    "            \n",
    "        self.__stop_words = stop_words\n",
    "        if stop_words is None:\n",
    "            self.__stop_words = {}\n",
    "            \n",
    "    def fit(self, raw_documents, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        return self.__transform(self.__process(raw_documents))\n",
    "    \n",
    "    def __process(self, raw_documents, y = None):\n",
    "        raw_documents = list(map(self.__preprocessor, raw_documents))\n",
    "        raw_documents = list(map(lambda x: ' '.join(list(filter(lambda x: x not in self.__stop_words, x.split()))), raw_documents))\n",
    "        return list(map(self.__tokenizer, raw_documents))\n",
    "          \n",
    "    def __transform(self, tokens: list):\n",
    "        vector_bag = []\n",
    "        for tt in tokens:\n",
    "            doc_vector = []\n",
    "            for t in tt:\n",
    "                if PretrainedVectorizer.ruscorpora_model.has_index_for(t):\n",
    "                    doc_vector.append(PretrainedVectorizer.ruscorpora_model.get_vector(t))\n",
    "            if len(doc_vector) == 0:\n",
    "                vector_bag.append(list(np.zeros((300))))\n",
    "            else:\n",
    "                vector_bag.append(list(np.sum(doc_vector, axis=0) / len(doc_vector))) \n",
    "        return np.array(vector_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb68165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, pipeline = None, params = {}):\n",
    "        self.__clf = GridSearchCV(\n",
    "            estimator=pipeline, \n",
    "            param_grid=params,\n",
    "            n_jobs=-1, verbose=1,\n",
    "        )\n",
    "        \n",
    "    def fit(self, raw_documents, y = None):\n",
    "        self.__clf.fit(raw_documents, y)\n",
    "        \n",
    "    def precision_recall_f1_support(self, raw_documents, y = None, average = None):\n",
    "        return precision_recall_fscore_support(\n",
    "            y, \n",
    "            self.__clf.predict(raw_documents),\n",
    "            average=average,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a70e9db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = TextData('data/new_eval_train.json')\n",
    "test_data = TextData('data/new_eval_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec8acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_binary = [\n",
    "    ('vect', CountVectorizer(\n",
    "        tokenizer=tokenize_spacy,\n",
    "        binary=True,\n",
    "        #stop_words=list(stop_words),               \n",
    "    ))\n",
    "]\n",
    "\n",
    "steps_count = [\n",
    "    ('vect', CountVectorizer(\n",
    "        tokenizer=tokenize_spacy,\n",
    "        #stop_words=list(stop_words),\n",
    "    ))\n",
    "]\n",
    "\n",
    "steps_tfidf = [\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize_spacy)),\n",
    "    ('tfidf', TfidfTransformer())\n",
    "]\n",
    "\n",
    "steps_emb = [\n",
    "    ('emb', EmbVectorizer(\n",
    "        tokenizer=tokenize_gensim, \n",
    "        stop_words=stop_words\n",
    "    )),\n",
    "]\n",
    "\n",
    "steps_pretrained = [\n",
    "    ('emb', PretrainedVectorizer(\n",
    "        tokenizer=tokenize_gensim_pretrained, \n",
    "        stop_words=stop_words\n",
    "    ))\n",
    "]\n",
    "\n",
    "best_parameters_sgd =  {\n",
    "    # params for clf\n",
    "    'clf__alpha': (0.0001,),\n",
    "    'clf__average': (True,),\n",
    "    'clf__eta0': (0.01,),\n",
    "    'clf__learning_rate': ('optimal',),\n",
    "    'clf__max_iter': (200, 250, 275, 1000,),\n",
    "    'clf__penalty': ('l2',),\n",
    "}\n",
    "\n",
    "optimization_parameters_sgd_lg = {\n",
    "    # params for clf\n",
    "    'clf__loss': ('hinge', 'modified_huber', 'squared_hinge' , 'log'),\n",
    "    'clf__learning_rate': ('optimal', 'invscaling', 'adaptive'),\n",
    "    'clf__alpha': (0.0001, 0.00001),\n",
    "    'clf__epsilon': (0.1, 0.01, 0.001),\n",
    "    'clf__penalty': ('l2', 'l1', 'elasticnet'),\n",
    "    'clf__max_iter': (275,1000,1500,2000),\n",
    "    'clf__average': (True, False),\n",
    "    'clf__eta0': (0.01, 0.001, 0.0001)\n",
    "}\n",
    "\n",
    "optimization_parameters_sgd_sm = {\n",
    "    # params for clf\n",
    "    'clf__loss': ( 'squared_hinge' ,),\n",
    "    'clf__learning_rate': ('optimal', 'adaptive'),\n",
    "    'clf__alpha': (0.0001, 0.00001),\n",
    "    'clf__epsilon': (0.1, 0.01),\n",
    "    'clf__penalty': ('l2', 'l1', 'elasticnet'),\n",
    "    'clf__max_iter': (250, 275, 1000),\n",
    "    'clf__average': (True, False),\n",
    "    'clf__eta0': (0.01, 0.001, 0.0001)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "496c931b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/conda_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6501681042253663, 0.653837743275749, 0.6288505591002959, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (pretrained emb)\n",
    "steps_sgd = steps_pretrained.copy()\n",
    "steps_sgd.append(('clf', SGDClassifier()))\n",
    "    \n",
    "sgd_pretrained_emb = Model(\n",
    "    pipeline = Pipeline(steps_sgd),\n",
    "    params = best_parameters_sgd,\n",
    ")\n",
    "\n",
    "sgd_pretrained_emb.fit(train_data.text, train_data.labels)\n",
    "sgd_pretrained_emb.precision_recall_f1_support(test_data.text, test_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18cfaab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.67      0.86      0.75      1890\n",
      "         0.0       0.64      0.29      0.40      1235\n",
      "         1.0       0.64      0.69      0.66      1448\n",
      "\n",
      "    accuracy                           0.65      4573\n",
      "   macro avg       0.65      0.61      0.60      4573\n",
      "weighted avg       0.65      0.65      0.63      4573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    test_data.labels,\n",
    "    sgd_pretrained_emb.__dict__['_Model__clf'].predict(test_data.text),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "720ae349",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/conda_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/conda_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3213979301524614, 0.4198556746118522, 0.2796664314613638, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (emb)\n",
    "steps_sgd = steps_emb.copy()\n",
    "steps_sgd.append(('clf', SGDClassifier()))\n",
    "    \n",
    "sgd_emb = Model(\n",
    "    pipeline = Pipeline(steps_sgd),\n",
    "    params = best_parameters_sgd,\n",
    ")\n",
    "\n",
    "sgd_emb.fit(train_data.text, train_data.labels)\n",
    "sgd_emb.precision_recall_f1_support(test_data.text, test_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb771947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.42      0.96      0.58      1890\n",
      "         0.0       1.00      0.00      0.00      1235\n",
      "         1.0       0.47      0.07      0.12      1448\n",
      "\n",
      "    accuracy                           0.42      4573\n",
      "   macro avg       0.63      0.34      0.24      4573\n",
      "weighted avg       0.59      0.42      0.28      4573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    test_data.labels,\n",
    "    sgd_emb.__dict__['_Model__clf'].predict(test_data.text),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ad26689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/conda_env/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/conda_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5881789514427936, 0.5982943363218893, 0.5870466519275348, None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (binary)\n",
    "steps_sgd = steps_binary.copy()\n",
    "steps_sgd.append(('clf', SGDClassifier()))\n",
    "\n",
    "sgd_binary = Model(\n",
    "    pipeline = Pipeline(steps_sgd),\n",
    "    params = best_parameters_sgd,\n",
    ")\n",
    "\n",
    "sgd_binary.fit(train_data.text, train_data.labels)\n",
    "sgd_binary.precision_recall_f1_support(test_data.text, test_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da094b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.67      0.73      0.70      1890\n",
      "         0.0       0.49      0.33      0.39      1235\n",
      "         1.0       0.56      0.66      0.61      1448\n",
      "\n",
      "    accuracy                           0.60      4573\n",
      "   macro avg       0.58      0.57      0.57      4573\n",
      "weighted avg       0.59      0.60      0.59      4573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    test_data.labels,\n",
    "    sgd_binary.__dict__['_Model__clf'].predict(test_data.text),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6862d008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/conda_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5905770121469165, 0.6020118084408485, 0.5883272977040023, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (count)\n",
    "steps_sgd = steps_count.copy()\n",
    "steps_sgd.append(('clf', SGDClassifier()))\n",
    "\n",
    "sgd_count = Model(\n",
    "    pipeline = Pipeline(steps_sgd),\n",
    "    params = best_parameters_sgd,\n",
    ")\n",
    "\n",
    "sgd_count.fit(train_data.text, train_data.labels)\n",
    "sgd_count.precision_recall_f1_support(test_data.text, test_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "422ba1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.66      0.74      0.70      1890\n",
      "         0.0       0.50      0.31      0.39      1235\n",
      "         1.0       0.58      0.67      0.62      1448\n",
      "\n",
      "    accuracy                           0.60      4573\n",
      "   macro avg       0.58      0.57      0.57      4573\n",
      "weighted avg       0.59      0.60      0.59      4573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    test_data.labels,\n",
    "    sgd_count.__dict__['_Model__clf'].predict(test_data.text),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c19da",
   "metadata": {},
   "source": [
    "clf_sgd.score(train_data.text, train_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63953e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/conda_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5896424824517709, 0.6015744587797944, 0.5885129181037644, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (tfidf)\n",
    "steps_sgd = steps_tfidf.copy()\n",
    "steps_sgd.append(('clf', SGDClassifier()))\n",
    "pipeline_sgd = Pipeline(steps_sgd)\n",
    "\n",
    "sgd_tfidf = Model(\n",
    "    pipeline = Pipeline(steps_sgd),\n",
    "    params = best_parameters_sgd,\n",
    ")\n",
    "\n",
    "sgd_tfidf.fit(train_data.text, train_data.labels)\n",
    "sgd_tfidf.precision_recall_f1_support(test_data.text, test_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f60eab77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.66      0.74      0.70      1890\n",
      "         0.0       0.50      0.32      0.39      1235\n",
      "         1.0       0.57      0.66      0.61      1448\n",
      "\n",
      "    accuracy                           0.60      4573\n",
      "   macro avg       0.58      0.57      0.57      4573\n",
      "weighted avg       0.59      0.60      0.59      4573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    test_data.labels,\n",
    "    sgd_tfidf.__dict__['_Model__clf'].predict(test_data.text),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e1030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
