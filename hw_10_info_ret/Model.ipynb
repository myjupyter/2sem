{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import pymorphy2\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21138d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\n",
    "    'stopwords',\n",
    "    download_dir=os.getcwd(),\n",
    ")\n",
    "nltk.data.path.append(os.getcwd())\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "stop_words.add('свой')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca206e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = '[^A-Za-z\\sаАбБвВгГдДеЕёЁжЖзЗиИйЙкКлЛмМнНоОпПрРсСтТуУфФхХцЦчЧшШщЩъЪыЫьЬэЭюЮяЯ]'\n",
    "regexRussian = '[^\\sаАбБвВгГдДеЕёЁжЖзЗиИйЙкКлЛмМнНоОпПрРсСтТуУфФхХцЦчЧшШщЩъЪыЫьЬэЭюЮяЯ]'\n",
    "rgRu = re.compile(regexRussian)\n",
    "rg = re.compile(regex)\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1186abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_convert_dict = {\n",
    "    'ADVB': 'ADV',\n",
    "    'ADJF': 'ADJ',\n",
    "    'NPRO': 'NOUN'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_condition(token):\n",
    "    return all([\n",
    "        token.ent_type == 0,\n",
    "        rgRu.match(token.text) is None,\n",
    "        token.is_alpha,\n",
    "        not token.like_num,\n",
    "        not token.like_email,\n",
    "    ])\n",
    "\n",
    "def tokenize_spacy(text: str):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if tokenization_condition(token):\n",
    "            token = morph.parse(token.text)[0].normal_form\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_gensim(text: str):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if tokenization_condition(token):\n",
    "            tag = morph.parse(token.text)[0]\n",
    "            token = tag.normal_form\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_gensim_pretrained(text: str):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if tokenization_condition(token):\n",
    "            tag = morph.parse(token.text)[0]\n",
    "            token = tag.normal_form\n",
    "            pos = tag.tag.POS\n",
    "            if pos_convert_dict.get(pos) is not None:\n",
    "                pos = pos_convert_dict[pos]\n",
    "            if pos is not None:\n",
    "                token = '_'.join([token, pos])\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "def process_fn(text: str):\n",
    "    doc = nlp(text)\n",
    "    text = re.sub(regex, \"\", text).lower()\n",
    "    new_text = []\n",
    "    for word in text.split(' '):\n",
    "        if word not in stop_words:\n",
    "            word = morph.parse(word)[0].normal_form\n",
    "            new_text.append(word)\n",
    "    text = ' '.join(new_text)\n",
    "    return text\n",
    "\n",
    "def process_fn_gensim(text: str):\n",
    "    text = ' '.join(re.sub(regexRussian, \"\", text).lower().split())\n",
    "    new_text = []\n",
    "    for word in text.split(' '):\n",
    "        if word not in stop_words:\n",
    "            tag = morph.parse(word)[0]\n",
    "            word = tag.normal_form\n",
    "            pos = tag.tag.POS\n",
    "            if pos is None:\n",
    "                continue\n",
    "            if pos == 'ADVB':\n",
    "                pos = 'ADV'\n",
    "            if pos == 'ADJF':\n",
    "                pos = 'ADJ'\n",
    "            word = '_'.join([word, pos])\n",
    "            new_text.append(word)\n",
    "    text = ' '.join(new_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7fecf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    def __init__(self, filepath, process_fn = process_fn):\n",
    "        with open(filepath) as file:\n",
    "            data = json.loads(file.read())\n",
    "        self.__labels = np.array([])\n",
    "        self.__raw_text = []\n",
    "        for d in data['data']:\n",
    "            self.__labels = np.append(self.__labels, d['evaluation'])\n",
    "            self.__raw_text.append(d['speech'])\n",
    "        \n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self.__labels\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        return self.__raw_text\n",
    "    \n",
    "    @property\n",
    "    def text_and_labels(self):\n",
    "        return list(zip(self.text, self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbVectorizer:\n",
    "    #ruscorpora_model = gensim.downloader.load('word2vec-ruscorpora-300')\n",
    "    def __init__(self, preprocessor = None, tokenizer = None, stop_words = None):\n",
    "        self.__preprocessor = preprocessor\n",
    "        if preprocessor is None:\n",
    "            self.__preprocessor = lambda x : x\n",
    "            \n",
    "        self.__tokenizer = tokenizer\n",
    "        if tokenizer is None:\n",
    "            self.__tokenizer = lambda x : x.split()\n",
    "            \n",
    "        self.__stop_words = stop_words\n",
    "        if stop_words is None:\n",
    "            self.__stop_words = {}\n",
    "            \n",
    "    def __process(self, raw_documents, y = None):\n",
    "        raw_documents = list(map(self.__preprocessor, raw_documents))\n",
    "        raw_documents = list(map(lambda x: ' '.join(list(filter(lambda x: x not in self.__stop_words, x.split()))), raw_documents))\n",
    "        tokens = list(map(self.__tokenizer, raw_documents))\n",
    "        return tokens\n",
    "        \n",
    "    def fit(self, raw_documents, y = None):\n",
    "        self.__fit(raw_documents)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, raw_documents):\n",
    "        return self.__transform(raw_documents)\n",
    "    \n",
    "    def __fit(self, raw_documents):\n",
    "        tokens = self.__process(raw_documents)\n",
    "        docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokens)]\n",
    "        self.__model = Doc2Vec(docs, vector_size=300, window=5, min_count=0, workers=4)\n",
    "        \n",
    "    def __transform(self, tokens: list):\n",
    "        vec = []\n",
    "        for token in self.__process(tokens):\n",
    "            vec.append(self.__model.infer_vector(token))\n",
    "        return np.array(vec)\n",
    "    \n",
    "    def __get_feature(self, tokens: list):\n",
    "        vector_bag = []\n",
    "        for t in tokens:\n",
    "            if EmbVectorizer.ruscorpora_model.has_index_for(t):\n",
    "                vector_bag.append(EmbVectorizer.ruscorpora_model.get_vector(t))\n",
    "            else:\n",
    "                print(t)\n",
    "        return np.sum(vector_bag, axis=0) / len(vector_bag)\n",
    "    \n",
    "    def features(self):\n",
    "        return self.__features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe2dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedVectorizer:\n",
    "    ruscorpora_model = gensim.downloader.load('word2vec-ruscorpora-300')\n",
    "    def __init__(self, preprocessor = None, tokenizer = None, stop_words = None):\n",
    "        self.__preprocessor = preprocessor\n",
    "        if preprocessor is None:\n",
    "            self.__preprocessor = lambda x : x\n",
    "            \n",
    "        self.__tokenizer = tokenizer\n",
    "        if tokenizer is None:\n",
    "            self.__tokenizer = lambda x : x.split()\n",
    "            \n",
    "        self.__stop_words = stop_words\n",
    "        if stop_words is None:\n",
    "            self.__stop_words = {}\n",
    "            \n",
    "    def fit(self, raw_documents, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        return self.__transform(self.__process(raw_documents))\n",
    "    \n",
    "    def __process(self, raw_documents, y = None):\n",
    "        raw_documents = list(map(self.__preprocessor, raw_documents))\n",
    "        raw_documents = list(map(lambda x: ' '.join(list(filter(lambda x: x not in self.__stop_words, x.split()))), raw_documents))\n",
    "        return list(map(self.__tokenizer, raw_documents))\n",
    "          \n",
    "    def __transform(self, tokens: list):\n",
    "        vector_bag = []\n",
    "        for tt in tokens:\n",
    "            doc_vector = []\n",
    "            for t in tt:\n",
    "                if PretrainedVectorizer.ruscorpora_model.has_index_for(t):\n",
    "                    doc_vector.append(PretrainedVectorizer.ruscorpora_model.get_vector(t))\n",
    "            if len(doc_vector) == 0:\n",
    "                vector_bag.append(list(np.zeros((300))))\n",
    "            else:\n",
    "                vector_bag.append(list(np.sum(doc_vector, axis=0) / len(doc_vector))) \n",
    "        return np.array(vector_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334dc46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, pipeline = None, params = {}):\n",
    "        self.__clf = GridSearchCV(\n",
    "            estimator=pipeline, \n",
    "            param_grid=params,\n",
    "            n_jobs=-1, verbose=1,\n",
    "        )\n",
    "        \n",
    "    def fit(self, raw_documents, y = None):\n",
    "        self.__clf.fit(raw_documents, y)\n",
    "        \n",
    "    def precision_recall_f1_support(self, raw_documents, y = None):\n",
    "        return precision_recall_fscore_support(\n",
    "            y, \n",
    "            self.__clf.predict(raw_documents),\n",
    "            average='weighted',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac7bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = TextData('data/new_eval_train.json')\n",
    "test_data = TextData('data/new_eval_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_binary = [\n",
    "    ('vect', CountVectorizer(\n",
    "        tokenizer=tokenize_spacy,\n",
    "        binary=True,\n",
    "        #stop_words=list(stop_words),               \n",
    "    ))\n",
    "]\n",
    "\n",
    "steps_count = [\n",
    "    ('vect', CountVectorizer(\n",
    "        tokenizer=tokenize_spacy,\n",
    "        #stop_words=list(stop_words),\n",
    "    ))\n",
    "]\n",
    "\n",
    "steps_tfidf = [\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize_spacy)),\n",
    "    ('tfidf', TfidfTransformer())\n",
    "]\n",
    "\n",
    "steps_emb = [\n",
    "    ('emb', EmbVectorizer(\n",
    "        tokenizer=tokenize_gensim, \n",
    "        stop_words=stop_words\n",
    "    )),\n",
    "]\n",
    "\n",
    "steps_pretrained = [\n",
    "    ('emb', PretrainedVectorizer(\n",
    "        tokenizer=tokenize_gensim_pretrained, \n",
    "        stop_words=stop_words\n",
    "    ))\n",
    "]\n",
    "\n",
    "best_parameters_sgd =  {\n",
    "    # params for clf\n",
    "    'clf__alpha': (0.0001,),\n",
    "    'clf__average': (True,),\n",
    "    'clf__eta0': (0.01,),\n",
    "    'clf__learning_rate': ('optimal',),\n",
    "    'clf__max_iter': (200, 250, 275, 1000,),\n",
    "    'clf__penalty': ('l2',),\n",
    "}\n",
    "\n",
    "optimization_parameters_sgd_lg = {\n",
    "    # params for clf\n",
    "    'clf__loss': ('hinge', 'modified_huber', 'squared_hinge' , 'log'),\n",
    "    'clf__learning_rate': ('optimal', 'invscaling', 'adaptive'),\n",
    "    'clf__alpha': (0.0001, 0.00001),\n",
    "    'clf__epsilon': (0.1, 0.01, 0.001),\n",
    "    'clf__penalty': ('l2', 'l1', 'elasticnet'),\n",
    "    'clf__max_iter': (275,1000,1500,2000),\n",
    "    'clf__average': (True, False),\n",
    "    'clf__eta0': (0.01, 0.001, 0.0001)\n",
    "}\n",
    "\n",
    "optimization_parameters_sgd_sm = {\n",
    "    # params for clf\n",
    "    'clf__loss': ( 'squared_hinge' ,),\n",
    "    'clf__learning_rate': ('optimal', 'adaptive'),\n",
    "    'clf__alpha': (0.0001, 0.00001),\n",
    "    'clf__epsilon': (0.1, 0.01),\n",
    "    'clf__penalty': ('l2', 'l1', 'elasticnet'),\n",
    "    'clf__max_iter': (250, 275, 1000),\n",
    "    'clf__average': (True, False),\n",
    "    'clf__eta0': (0.01, 0.001, 0.0001)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fc23344a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/conda_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6493056416446652, 0.6529630439536409, 0.6275007707304155, None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (pretrained emb)\n",
    "steps_sgd = steps_pretrained.copy()\n",
    "steps_sgd.append(('clf', SGDClassifier()))\n",
    "    \n",
    "sgd_emb_model = Model(\n",
    "    pipeline = Pipeline(steps_sgd),\n",
    "    params = best_parameters_sgd,\n",
    ")\n",
    "\n",
    "sgd_emb_model.fit(train_data.text, train_data.labels)\n",
    "sgd_emb_model.precision_recall_f1_support(test_data.text, test_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1c527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/conda_env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (pretrained emb)\n",
    "steps_sgd = steps_emb.copy()\n",
    "steps_sgd.append(('clf', SGDClassifier()))\n",
    "    \n",
    "sgd_emb_model = Model(\n",
    "    pipeline = Pipeline(steps_sgd),\n",
    "    params = best_parameters_sgd,\n",
    ")\n",
    "\n",
    "sgd_emb_model.fit(train_data.text, train_data.labels)\n",
    "sgd_emb_model.precision_recall_f1_support(test_data.text, test_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644dc104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent (binary)\n",
    "steps_sgd = steps_binary.copy()\n",
    "steps_sgd.append(('clf', SGDClassifier()))\n",
    "pipeline_sgd = Pipeline(steps_sgd)\n",
    "\n",
    "parameters_sgd = best_parameters_sgd \n",
    "\n",
    "clf_sgd = GridSearchCV(\n",
    "    estimator=pipeline_sgd, \n",
    "    param_grid=parameters_sgd,\n",
    "    n_jobs=-1, verbose=1,\n",
    ")\n",
    "\n",
    "clf_sgd.fit(train_data.text, train_data.labels)\n",
    "precision_recall_fscore_support(\n",
    "    test_data.labels, \n",
    "    clf_sgd.predict(test_data.text),\n",
    "    average='weighted',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f226c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent (count)\n",
    "steps_sgd = steps_count.copy()\n",
    "steps_sgd.append(('clf', SGDClassifier()))\n",
    "pipeline_sgd = Pipeline(steps_sgd)\n",
    "\n",
    "parameters_sgd = best_parameters_sgd\n",
    "\n",
    "clf_sgd = GridSearchCV(\n",
    "    estimator=pipeline_sgd, \n",
    "    param_grid=parameters_sgd,\n",
    "    n_jobs=-1, verbose=1,\n",
    ")\n",
    "\n",
    "clf_sgd.fit(train_data.text, train_data.labels)\n",
    "precision_recall_fscore_support(\n",
    "    test_data.labels, \n",
    "    clf_sgd.predict(test_data.text),\n",
    "    average='weighted',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb9614",
   "metadata": {},
   "source": [
    "clf_sgd.score(train_data.text, train_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccdfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent (tfidf)\n",
    "steps_sgd = steps_tfidf.copy()\n",
    "steps_sgd.append(('clf', SGDClassifier()))\n",
    "pipeline_sgd = Pipeline(steps_sgd)\n",
    "\n",
    "parameters = best_parameters_sgd\n",
    "\n",
    "clf_sgd = GridSearchCV(\n",
    "    estimator=pipeline_sgd, \n",
    "    param_grid=parameters,\n",
    "    n_jobs=-1, verbose=1,\n",
    ")\n",
    "\n",
    "clf_sgd.fit(train_data.text, train_data.labels)\n",
    "precision_recall_fscore_support(\n",
    "    test_data.labels, \n",
    "    clf_sgd.predict(test_data.text),\n",
    "    average='weighted',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b393811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
